{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "my_data = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of legacy questions to train/test the model. Query loads questions where last modification date was last year.\n",
    "\n",
    "Stackexchange query is https://data.stackexchange.com/stackoverflow/query/edit/847084\n",
    "\n",
    "select Id, Score, ViewCount, CreationDate, LastActivityDate, title, tags, body\n",
    "from Posts \n",
    "where (score > 100) and (LastActivityDate > '2017-04-01') \n",
    "and (LastActivityDate < '2018-04-01') and (PostTypeId = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15089, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load data into Pandas dataframe\n",
    "datafile = \"QueryResultsOld.csv\"\n",
    "full_path = os.path.join(my_data, datafile)\n",
    "df_questions = pd.read_csv(full_path)\n",
    "print(df_questions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code cleans up the HTML content, removes stop words and converts the remaining ones into a list of stems.\n",
    "The result is a new column in the table with that list for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take only aplanumeric words, no punctuation signs\n",
    "tokenizer = nltk.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Prepare set of stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Define stemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "wordsFiltered = []\n",
    "wordsArray = []\n",
    "\n",
    "for html_text in df_questions['body'] + \" \" + df_questions['title']:\n",
    "    soup = BeautifulSoup(html_text, \"lxml\").get_text()\n",
    "    words = tokenizer.tokenize(soup.lower())\n",
    "    his_words = ''\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            stem = snowball_stemmer.stem(w)\n",
    "            wordsFiltered.append(stem)\n",
    "            his_words = his_words + ' ' + stem\n",
    "    wordsArray.append(his_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a column to the dataframe with the list of cleaned stems\n",
    "df_questions['words'] = wordsArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of stems gets vectorized. Each question is a line of the matrix and each stem is a column. The values are the number of occurrences of each stem in each question.\n",
    "Any word that is used in more than 95% of the questions or less than 5 times across all questions is removed, because its either too common or too specific to be used for the topic determination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stem_vectorizer = CountVectorizer(lowercase = True, ngram_range=(1, 2), max_df=0.95, min_df=5)\n",
    "stem_matrix = stem_vectorizer.fit_transform(df_questions['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15089, 25507)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorize all the tags used in the training set\n",
    "tag_vectorizer = CountVectorizer(lowercase = True, max_df=1.0, min_df=0, token_pattern = '[^<>]+')\n",
    "tag_matrix = tag_vectorizer.fit_transform(df_questions['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a datraframe with the list of tags and the number of times they are used\n",
    "tag_names = tag_vectorizer.get_feature_names()\n",
    "\n",
    "tag_df = pd.DataFrame(tag_matrix.sum(0)).T\n",
    "tag_df.rename(index=str, columns={0:'number'}, inplace=True)\n",
    "tag_df['names'] = tag_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, a multi-label SVC is used to get the probability for each question to have a given tag.<br>Features are the stem matriw per question.<br>Labels are the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting 08:04:53\n",
      "End fitting 16:23:41\n",
      "Fitting duration 8:18:48.785657\n"
     ]
    }
   ],
   "source": [
    "X = stem_matrix\n",
    "Y = tag_matrix\n",
    "\n",
    "MultiLabelClassif = OneVsRestClassifier(SVC(kernel='linear', probability=True))\n",
    "start = dt.datetime.now()\n",
    "print(\"Start fitting {0}\".format(start.strftime(\"%H:%M:%S\")))\n",
    "MultiLabelClassif.fit(X, Y)\n",
    "end = dt.datetime.now()\n",
    "print(\"End fitting {0}\".format(end.strftime(\"%H:%M:%S\")))\n",
    "print(\"Fitting duration {0}\".format(end - start) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dump the trained vectorizer\n",
    "output = open('stem_vectorizer.pkl', 'wb')\n",
    "pickle.dump(stem_vectorizer, output, -1)\n",
    "output.close()\n",
    "\n",
    "# dump the trained classifier\n",
    "output = open('MultiLabelClassif.pkl', 'wb')\n",
    "pickle.dump(MultiLabelClassif, output, -1)\n",
    "output.close()\n",
    "\n",
    "# dump the tags dataframe\n",
    "output = open('tag_df.pkl', 'wb')\n",
    "pickle.dump(tag_df, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
